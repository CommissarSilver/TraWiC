{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63790a99-0c34-4474-a5db-0eb24f004190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamaj/scratch/twmc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "logger = logging.getLogger(\"model\")\n",
    "\n",
    "\n",
    "class MistralCoder:\n",
    "    def __init__(self) -> None:\n",
    "        # toknizer config\n",
    "        self.FIM_PREFIX = \"<fim-prefix>\"\n",
    "        self.FIM_MIDDLE = \"<fim-middle>\"\n",
    "        self.FIM_SUFFIX = \"<fim-suffix>\"\n",
    "        self.FIM_PAD = \"<fim-pad>\"\n",
    "        self.ENDOFTEXT = \"<|endoftext|>\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"/home/vamaj/scratch/TraWiC/llms/mistral\",\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True,\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = (\n",
    "            \"right\"  # Fix weird overflow issue with fp16 training\n",
    "        )\n",
    "        self.tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"additional_special_tokens\": [\n",
    "                    self.FIM_PREFIX,\n",
    "                    self.FIM_MIDDLE,\n",
    "                    self.FIM_SUFFIX,\n",
    "                    self.FIM_PAD,\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        # model config\n",
    "        self.device_map = {\"\": 0}\n",
    "        base_model_path = \"/home/vamaj/scratch/TraWiC/llms/mistral\"\n",
    "        adapter_path = \"/home/vamaj/scratch/TraWiC/llms/mistral_fim\"\n",
    "        try:\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_path,\n",
    "                device_map=self.device_map,\n",
    "                local_files_only=True,\n",
    "            )\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                adapter_path,\n",
    "            )\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            logger.info(f\"Mistral-FIM model successfuly loaded\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in loading the Mistral-FIM model\")\n",
    "            raise Exception(\"Problem in initializing Mistral-FIM Model\")\n",
    "\n",
    "    def predict(self, input_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate code snippet from the input text\n",
    "\n",
    "        Args:\n",
    "            input_text (str): input code. not tokenized.\n",
    "\n",
    "        Raises:\n",
    "            e: any error in generating code snippet\n",
    "\n",
    "        Returns:\n",
    "            str: geenrated code snippet\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inputs: torch.Tensor = self.tokenizer.encode(\n",
    "                input_text, return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs: torch.Tensor = self.model.generate(inputs)\n",
    "\n",
    "            logger.debug(\n",
    "                f\"Mistral-FIM Invoked - input = ( {input_text} ) - output = ( {self.tokenizer.decode(outputs[0])} )\"\n",
    "            )\n",
    "            return self.tokenizer.decode(outputs[0])\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in generating code snippet from Mistral-FIM\")\n",
    "            raise e\n",
    "\n",
    "    def extract_fim_part(self, s: str):\n",
    "        \"\"\"\n",
    "        Find the index of <fim-middle>\n",
    "\n",
    "        Args:\n",
    "            s (str): input string\n",
    "\n",
    "        Raises:\n",
    "            e: any excepetion\n",
    "\n",
    "        Returns:\n",
    "            _type_: fim part of the input string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start = s.find(self.FIM_MIDDLE) + len(self.FIM_MIDDLE)\n",
    "            stop = s.find(self.ENDOFTEXT, start) or len(s)\n",
    "            return s[start:stop]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in extracting fim part from Mistral-FIM output\")\n",
    "            raise e\n",
    "\n",
    "    def infill(\n",
    "        self,\n",
    "        prefix_suffix_tuples: Tuple[str, str, str, str],\n",
    "        max_tokens: int = 200,\n",
    "        temperature: float = 0.8,\n",
    "        top_p: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate code snippets by infilling between the prefix and suffix.\n",
    "\n",
    "        Args:\n",
    "            prefix_suffix_tuples (_type_): a tuple of form (prefix, suffix)\n",
    "            max_tokens (int, optional): maximum tokens for the model. Defaults to 200.\n",
    "            temperature (float, optional): model temp. Defaults to 0.8.\n",
    "            top_p (float, optional): top_p. Defaults to 0.95.\n",
    "\n",
    "        Returns:\n",
    "            str: infilled code snippet\n",
    "        \"\"\"\n",
    "        output_list = True\n",
    "        if type(prefix_suffix_tuples) == tuple:\n",
    "            prefix_suffix_tuples = [prefix_suffix_tuples]\n",
    "            output_list = False\n",
    "\n",
    "        prompts = [\n",
    "            f\"{self.FIM_PREFIX}{prefix}{self.FIM_SUFFIX}{suffix}{self.FIM_MIDDLE}\"\n",
    "            for infill_obj, prefix, suffix, level in prefix_suffix_tuples\n",
    "        ]\n",
    "        # `return_token_type_ids=False` is essential, or we get nonsense output.\n",
    "        inputs = self.tokenizer(\n",
    "            prompts, return_tensors=\"pt\", padding=True, return_token_type_ids=False\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        max_length = inputs.input_ids[0].size(0) + max_tokens\n",
    "        if max_length > 2048:\n",
    "            # dp not even try to generate if the input is too long\n",
    "            return \"too_many_tokens\"\n",
    "        with torch.no_grad():\n",
    "            x = len(prefix_suffix_tuples[0][0])\n",
    "            try:\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    do_sample=True,\n",
    "                    top_p=top_p,\n",
    "                    temperature=temperature,\n",
    "                    max_length=max_length,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                if type(e) == IndexError:\n",
    "                    logger.exception(\n",
    "                        f\"Error in generating code snippet from Mistral-FIM with an IndexError.\",\n",
    "                    )\n",
    "                    return \"too_many_tokens\"\n",
    "                else:\n",
    "                    logger.exception(\n",
    "                        f\"Error in generating code snippet from Mistral-FIM {e}\"\n",
    "                    )\n",
    "                outputs = None\n",
    "        try:\n",
    "            if outputs != None:\n",
    "                result = [\n",
    "                    self.extract_fim_part(\n",
    "                        self.tokenizer.decode(tensor, skip_special_tokens=False)\n",
    "                    )\n",
    "                    for tensor in outputs\n",
    "                ]\n",
    "                logger.debug(\n",
    "                    f\"Mistral-FIM Invoked - input = ( {prefix_suffix_tuples} ) - output = {result}\"\n",
    "                )\n",
    "                return result if output_list else result[0]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in generating code snippet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf5a4d7-a369-4ecf-827c-eacda3a13e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.43s/it]\n"
     ]
    }
   ],
   "source": [
    "coder=MistralCoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b8605b-b970-4e2a-b03d-3fa70e1b6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=coder.infill(\n",
    "            ('heelo',\n",
    "                'def print_my_name',\n",
    "                'return',\n",
    "                'function',\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1270b4c4-4655-4dd1-b566-a63de341e86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' my_name'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac6997-76a3-4e33-92d6-1239b2e6ea30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 TWMC Kernel",
   "language": "python",
   "name": "twmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
