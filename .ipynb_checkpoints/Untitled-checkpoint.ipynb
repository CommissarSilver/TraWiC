{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63790a99-0c34-4474-a5db-0eb24f004190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "logger = logging.getLogger(\"model\")\n",
    "\n",
    "\n",
    "class MistralCoder:\n",
    "    def __init__(self) -> None:\n",
    "        # toknizer config\n",
    "        self.FIM_PREFIX = \"<fim-prefix>\"\n",
    "        self.FIM_MIDDLE = \"<fim-middle>\"\n",
    "        self.FIM_SUFFIX = \"<fim-suffix>\"\n",
    "        self.FIM_PAD = \"<fim-pad>\"\n",
    "        self.ENDOFTEXT = \"<|endoftext|>\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"/home/vamaj/scratch/TraWiC/llms/mistral\",\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True,\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = (\n",
    "            \"right\"  # Fix weird overflow issue with fp16 training\n",
    "        )\n",
    "        self.tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"additional_special_tokens\": [\n",
    "                    self.FIM_PREFIX,\n",
    "                    self.FIM_MIDDLE,\n",
    "                    self.FIM_SUFFIX,\n",
    "                    self.FIM_PAD,\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        # model config\n",
    "        self.device_map = {\"\": 0}\n",
    "        base_model_path = \"/home/vamaj/scratch/TraWiC/llms/mistral\"\n",
    "        adapter_path = \"/home/vamaj/scratch/TraWiC/llms/mistral_fim\"\n",
    "        try:\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_path,\n",
    "                device_map=self.device_map,\n",
    "                local_files_only=True,\n",
    "            )\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                adapter_path,\n",
    "            )\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            logger.info(f\"Mistral-FIM model successfuly loaded\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in loading the Mistral-FIM model\")\n",
    "            raise Exception(\"Problem in initializing Mistral-FIM Model\")\n",
    "\n",
    "    def predict(self, input_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate code snippet from the input text\n",
    "\n",
    "        Args:\n",
    "            input_text (str): input code. not tokenized.\n",
    "\n",
    "        Raises:\n",
    "            e: any error in generating code snippet\n",
    "\n",
    "        Returns:\n",
    "            str: geenrated code snippet\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inputs: torch.Tensor = self.tokenizer.encode(\n",
    "                input_text, return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs: torch.Tensor = self.model.generate(inputs)\n",
    "\n",
    "            logger.debug(\n",
    "                f\"Mistral-FIM Invoked - input = ( {input_text} ) - output = ( {self.tokenizer.decode(outputs[0])} )\"\n",
    "            )\n",
    "            return self.tokenizer.decode(outputs[0])\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in generating code snippet from Mistral-FIM\")\n",
    "            raise e\n",
    "\n",
    "    def extract_fim_part(self, s: str):\n",
    "        \"\"\"\n",
    "        Find the index of <fim-middle>\n",
    "\n",
    "        Args:\n",
    "            s (str): input string\n",
    "\n",
    "        Raises:\n",
    "            e: any excepetion\n",
    "\n",
    "        Returns:\n",
    "            _type_: fim part of the input string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start = s.find(self.FIM_MIDDLE) + len(self.FIM_MIDDLE)\n",
    "            stop = s.find(self.ENDOFTEXT, start) or len(s)\n",
    "            return s[start:stop]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in extracting fim part from Mistral-FIM output\")\n",
    "            raise e\n",
    "\n",
    "    def infill(\n",
    "        self,\n",
    "        prefix_suffix_tuples: Tuple[str, str, str, str],\n",
    "        max_tokens: int = 200,\n",
    "        temperature: float = 0.8,\n",
    "        top_p: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate code snippets by infilling between the prefix and suffix.\n",
    "\n",
    "        Args:\n",
    "            prefix_suffix_tuples (_type_): a tuple of form (prefix, suffix)\n",
    "            max_tokens (int, optional): maximum tokens for the model. Defaults to 200.\n",
    "            temperature (float, optional): model temp. Defaults to 0.8.\n",
    "            top_p (float, optional): top_p. Defaults to 0.95.\n",
    "\n",
    "        Returns:\n",
    "            str: infilled code snippet\n",
    "        \"\"\"\n",
    "        output_list = True\n",
    "        if type(prefix_suffix_tuples) == tuple:\n",
    "            prefix_suffix_tuples = [prefix_suffix_tuples]\n",
    "            output_list = False\n",
    "\n",
    "        prompts = [\n",
    "            f\"{self.FIM_PREFIX}{prefix}{self.FIM_SUFFIX}{suffix}{self.FIM_MIDDLE}\"\n",
    "            for infill_obj, prefix, suffix, level in prefix_suffix_tuples\n",
    "        ]\n",
    "        # `return_token_type_ids=False` is essential, or we get nonsense output.\n",
    "        inputs = self.tokenizer(\n",
    "            prompts, return_tensors=\"pt\", padding=True, return_token_type_ids=False\n",
    "        ).to('cuda')\n",
    "\n",
    "        max_length = inputs.input_ids[0].size(0) + max_tokens\n",
    "        if max_length > 2048:\n",
    "            # dp not even try to generate if the input is too long\n",
    "            return \"too_many_tokens\"\n",
    "        with torch.no_grad():\n",
    "            x = len(prefix_suffix_tuples[0][0])\n",
    "            try:\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    do_sample=True,\n",
    "                    top_p=top_p,\n",
    "                    temperature=temperature,\n",
    "                    max_length=max_length,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                if type(e) == IndexError:\n",
    "                    logger.exception(\n",
    "                        f\"Error in generating code snippet from Mistral-FIM with an IndexError.\",\n",
    "                    )\n",
    "                    return \"too_many_tokens\"\n",
    "                else:\n",
    "                    logger.exception(\n",
    "                        f\"Error in generating code snippet from Mistral-FIM {e}\"\n",
    "                    )\n",
    "                outputs = None\n",
    "        try:\n",
    "            if outputs != None:\n",
    "                result = [\n",
    "                    self.extract_fim_part(\n",
    "                        self.tokenizer.decode(tensor, skip_special_tokens=False)\n",
    "                    )\n",
    "                    for tensor in outputs\n",
    "                ]\n",
    "                logger.debug(\n",
    "                    f\"Mistral-FIM Invoked - input = ( {prefix_suffix_tuples} ) - output = {result}\"\n",
    "                )\n",
    "                return result if output_list else result[0]\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error in generating code snippet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf5a4d7-a369-4ecf-827c-eacda3a13e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.95s/it]\n"
     ]
    }
   ],
   "source": [
    "coder=MistralCoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b8605b-b970-4e2a-b03d-3fa70e1b6841",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MistralCoder' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m j\u001b[38;5;241m=\u001b[39m\u001b[43mcoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheelo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdef print_my_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 141\u001b[0m, in \u001b[0;36mMistralCoder.infill\u001b[0;34m(self, prefix_suffix_tuples, max_tokens, temperature, top_p)\u001b[0m\n\u001b[1;32m    134\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFIM_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFIM_SUFFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFIM_MIDDLE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m infill_obj, prefix, suffix, level \u001b[38;5;129;01min\u001b[39;00m prefix_suffix_tuples\n\u001b[1;32m    137\u001b[0m ]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# `return_token_type_ids=False` is essential, or we get nonsense output.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    140\u001b[0m     prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m    143\u001b[0m max_length \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39minput_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m max_tokens\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2048\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# dp not even try to generate if the input is too long\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MistralCoder' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "j=coder.infill(\n",
    "            ('heelo',\n",
    "                'def print_my_name',\n",
    "                'return',\n",
    "                'function',\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270b4c4-4655-4dd1-b566-a63de341e86e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 TWMC Kernel",
   "language": "python",
   "name": "twmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
